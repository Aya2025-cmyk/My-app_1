{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f64cd159-7095-4018-a87f-a5129fb07f60",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<string>:14\u001b[1;36m\u001b[0m\n\u001b[1;33m    donne = []\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Fonction de scraping des données\n",
    "def scrape_data(page_index, category):\n",
    "    url= 'https://sn.coinafrique.com/categorie/vetements-enfants'\n",
    "    res= get(url)#recupérer le code html de la page\n",
    "    soup= bs(res.text, 'html.parser')\n",
    "    containers= soup.find_all('div', class_=\"col s6 m4 l3\")\n",
    "\n",
    "    # Logique pour extraire les données vetements des enfants\n",
    "  donne = []\n",
    "for container in containers :\n",
    "    try :\n",
    "         type_habits=container.find(\"p\", class_=\"ad__card-description\").text.strip()\n",
    "         Prix=container.find(\"p\", class_=\"ad__card-price\").text.strip().replace(\"CFA\", \"\" )\n",
    "         Adresse=container.find(\"p\", class_=\"ad__card-location\").text.strip().replace(\"location_on\", \"\")\n",
    "         Img_link=container.find(\"img\",class_=\"ad__card-img\")[\"src\"]\n",
    "        \n",
    "         dic = {\n",
    "                \"Type habits\": type_habits,\n",
    "                \"Prix\": Prix,\n",
    "                \"Adresse\": Adresse,\n",
    "                \"Image_lien\": Image_lien,   \n",
    "            }\n",
    "         donne.append(dic)\n",
    "    except :\n",
    "           pass\n",
    "Df=pd.DataFrame(donne)\n",
    "#generalisation des autres pages\n",
    "DF=pd.DataFrame(donne)\n",
    "for index in range(1,21):\n",
    "    url= f'https://sn.coinafrique.com/categorie/vetements-enfants?page={index}'\n",
    "    res= get(url)\n",
    "    soup= bs(res.text, 'html.parser')\n",
    "    containers= soup.find_all('div', class_=\"col s6 m4 l3\")\n",
    "    donne = []\n",
    "for container in containers :\n",
    "    try :\n",
    "         type_habits=container.find(\"p\", class_=\"ad__card-description\").text.strip()\n",
    "         Prix=container.find(\"p\", class_=\"ad__card-price\").text.strip().replace(\"CFA\", \"\" )\n",
    "         Adresse=container.find(\"p\", class_=\"ad__card-location\").text.strip().replace(\"location_on\", \"\")\n",
    "         Image_lien=container.find(\"img\",class_=\"ad__card-img\")[\"src\"]\n",
    "        \n",
    "         dic = {\n",
    "                \"Type habits\": type_habits,\n",
    "                \"Prix\": Prix,\n",
    "                \"Adresse\": Adresse,\n",
    "                \"Image_lien\": Image_lien,   \n",
    "            }\n",
    "         donne.append(dic)\n",
    "    except :\n",
    "           pass\n",
    "Df=pd.DataFrame(donne)\n",
    "DF= pd.concat([DF,Df], axis=0).reset_index(drop=True)\n",
    "#importation des contenaires\n",
    "def scrape_data(page_index, category):\n",
    "\n",
    "         url= 'https://sn.coinafrique.com/categorie/chaussures-enfants'\n",
    "         res= get(url)#recupérer le code html de la page\n",
    "         soup= bs(res.text, 'html.parser')\n",
    "         Paquets= soup.find_all('div', class_=\"col s6 m4 l3\")\n",
    "#extraire les données des chaussres des enfants\n",
    "     data= []\n",
    "for Paquet in Paquets:\n",
    "    try :\n",
    "        \n",
    "        Type_chaussures=Paquet.find(\"p\", class_=\"ad__card-description\").text.strip()\n",
    "        Prix=Paquet.find(\"p\",class_=\"ad__card-price\").text.strip().replace(\"CFA\", \"\")\n",
    "        Adresse=Paquet.find(\"p\", class_=\"ad__card-location\").text.replace(\"location_on\", \"\")\n",
    "        Image_lien=Paquet.find(\"img\", class_=\"ad__card-img\")[\"src\"]\n",
    "        dic= {\n",
    "            \"Type chaussure\":Type_chaussures,\n",
    "            \"Prix\": Prix,\n",
    "            \"Adresse\": Adresse,\n",
    "            \"Image lien\":Image_lien,\n",
    "        }\n",
    "        data.append(dic)\n",
    "    except :\n",
    "          pass\n",
    "DF1=pd.DataFrame(data)\n",
    "#Generaliser sur les autres pages\n",
    "DF2=pd.DataFrame(data)\n",
    "for p_index in range(1,21):\n",
    "    url= f'https://sn.coinafrique.com/categorie/chaussures-enfants?page={p_index}'\n",
    "    res= get(url)\n",
    "    soup= bs(res.text, 'html.parser')\n",
    "    Paquets= soup.find_all('div', class_=\"col s6 m4 l3\")\n",
    "for Paquet in Paquets:\n",
    "    try :\n",
    "        \n",
    "        Type_chaussures=Paquet.find(\"p\", class_=\"ad__card-description\").text.strip()\n",
    "        Prix=Paquet.find(\"p\",class_=\"ad__card-price\").text.strip().replace(\"CFA\", \"\")\n",
    "        Adresse=Paquet.find(\"p\", class_=\"ad__card-location\").text.replace(\"location_on\", \"\")\n",
    "        Image_lien=Paquet.find(\"img\", class_=\"ad__card-img\")[\"src\"]\n",
    "        dic= {\n",
    "            \"Type chaussure\":Type_chaussures,\n",
    "            \"Prix\": Prix,\n",
    "            \"Adresse\": Adresse,\n",
    "            \"Image lien\":Image_lien,\n",
    "        }\n",
    "        data.append(dic)\n",
    "    except :\n",
    "          pass\n",
    "DF1=pd.DataFrame(data)\n",
    "DF3= pd.concat([DF2,DF1], axis=0).reset_index(drop=True)\n",
    "# Interface Streamlit\n",
    "st.title(\"MY DATA SCRAPER APP\")\n",
    "st.markdown(\"This app performs web scraping of data from Dakar-Auto over multiple pages. You can also download scraped data directly from the app.\")\n",
    "\n",
    "# Sélection des pages à scraper\n",
    "page_index = st.selectbox(\"Pages indexes\", options=[for i in range(1,21)])\n",
    "category = st.selectbox(\"Options\", options=[\"Vetements enfants\", \"Chaussures enfants\"])\n",
    "\n",
    "# Boutons pour scraper et afficher les données\n",
    "if st.button(f\"Scrape {category} data\"):\n",
    "    with st.spinner('Scraping data...'):\n",
    "        data = scrape_data(page_index, category)\n",
    "        st.success(f\"Data scraped successfully for {category} on page {page_index}\")\n",
    "        \n",
    "        # Afficher les données\n",
    "        st.write(data)\n",
    "        \n",
    "        # Option pour télécharger les données\n",
    "        csv = data.to_csv(index=False)\n",
    "        st.download_button(\"Download CSV\", csv, \"scraped_data.csv\", \"text/csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c618d7-60b6-4e93-ab0c-6be6cefeb9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
